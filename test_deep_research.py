#!/usr/bin/env python3
"""
Asyncio Parallel Streaming Deep Research Report Generator

Requirements:
  - Python 3.8+
  - pip install --upgrade openai

Set environment variable:
  export OPENAI_API_KEY="YOUR_API_KEY"
"""

import os
import asyncio
import openai
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) í™˜ê²½ ë³€ìˆ˜ & í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = os.getenv("OPENAI_API_KEY") or ""
if not openai.api_key:
    raise RuntimeError("ERROR: OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")

client = openai.OpenAI()  # async ì§€ì› í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì„¹ì…˜ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sections = [
    ("ì‹œì¥ ë™í–¥ ë¶„ì„",     "ì‹œì¥ ë™í–¥ ë¶„ì„ì— ëŒ€í•œ ì‹¬ì¸µ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”."),
    ("ê²½ìŸì‚¬ ë¹„êµ",       "ì£¼ìš” ê²½ìŸì‚¬ ë¹„êµ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."),
    ("ê¸°ìˆ ì  ì ‘ê·¼ ë°©ë²•",   "í•´ë‹¹ ì£¼ì œì˜ ê¸°ìˆ ì  ì ‘ê·¼ ë°©ë²•ì„ ì„¤ëª…í•˜ì„¸ìš”."),
    ("êµ¬í˜„ ì•„í‚¤í…ì²˜ ì„¤ê³„", "ì í•©í•œ êµ¬í˜„ ì•„í‚¤í…ì²˜ ì„¤ê³„ì•ˆì„ ì œì•ˆí•˜ì„¸ìš”."),
    ("ë¹„ìš© ë° ROI ë¶„ì„",   "ë¹„ìš© ë¶„ì„ ë° ì˜ˆìƒ ROIë¥¼ ê³„ì‚°í•˜ì„¸ìš”."),
    ("ê²°ë¡  ë° ì œì•ˆ",      "ì „ì²´ ìš”ì•½ ë° ìµœì¢… ì œì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.")
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) íˆ´ ì„¤ì • (file_search ì œê±°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tools = [
    {"type": "web_search_preview"},
    {"type": "code_interpreter", "container": {"type": "auto"}}
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ì´ë²¤íŠ¸ ì €ì¥ì†Œ (task_id â†’ ë¦¬ìŠ¤íŠ¸ of dict)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
events_store: dict[int, list[dict]] = {}

async def run_stream(task_id: int, title: str, prompt: str):
    # 3-1) ì €ì¥ì†Œ ì´ˆê¸°í™”
    events_store[task_id] = []

    try:
        # 3-2) ìŠ¤íŠ¸ë¦¼ ì—´ê¸° (client.responses.create ì‚¬ìš©)
        stream = await client.responses.create(
            model="o3-deep-research",
            input=[
                {"role":"system","content":[{"type":"input_text","text":"You are a research assistant."}]},
                {"role":"user",  "content":[{"type":"input_text","text":prompt}]}
            ],
            tools=tools,
            reasoning={"summary":"auto"},
            stream=True,
        )

        # 3-3) ì´ë²¤íŠ¸ ìˆœíšŒ
        async for evt in stream:
            etype = evt.get("type", "")
            data = evt.get("data", {})
            # ì‹œê°„Â·íƒ€ì…Â·í˜ì´ë¡œë“œ ê¸°ë¡
            evt_record = {
                "time": datetime.utcnow().isoformat(),
                "type": etype,
                **data
            }
            events_store[task_id].append(evt_record)

            # ì½˜ì†”ì— ì´ë²¤íŠ¸ë³„ ì¶œë ¥
            if etype == "response.tool_start":
                print(f"[{task_id}] ğŸ”§ tool_start â†’", data.get("tool"))
            elif etype in ("response.tool_response", "response.tool_output"):
                out = data.get("output", "")
                print(f"[{task_id}] ğŸ“¥ tool_response â†’", str(out)[:100].replace("\n"," "))
            elif etype == "response.output_text.delta":
                print(f"[{task_id}] âœ‰ï¸", data.get("delta",""), end="", flush=True)
            elif etype == "response.message_end":
                print(f"\n[{task_id}] âœ… message_end")
            else:
                # reasoning, plan, error ë“±
                print(f"[{task_id}] ğŸ“Œ {etype}")

    except Exception as e:
        print(f"[{task_id}] âŒ Error during stream:", e)

    finally:
        # 3-4) ìŠ¤íŠ¸ë¦¼ ë‹«ê¸°
        try:
            await stream.aclose()
        except:
            pass
        count = len(events_store.get(task_id, []))
        print(f"[{task_id}] {title} ì™„ë£Œ, ì´ë²¤íŠ¸ ìˆ˜ì§‘: {count}ê°œ")
        return task_id, title

async def main():
    # 4) create_task + gather ë¡œ ë³‘ë ¬ ì‹¤í–‰
    tasks = [
        asyncio.create_task(run_stream(i+1, sec[0], sec[1]))
        for i, sec in enumerate(sections)
    ]
    results = await asyncio.gather(*tasks)

    # 5) ê° task ì˜ ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ ì¶œë ¥
    for task_id, title in sorted(results):
        print(f"\n=== Task {task_id}: {title} ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ ===")
        for e in events_store.get(task_id, []):
            print(e)

if __name__ == "__main__":
    asyncio.run(main())
